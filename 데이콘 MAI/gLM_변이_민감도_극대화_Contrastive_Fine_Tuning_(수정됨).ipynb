{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# 구글 코랩에서 실행 가능하며, `test.csv`와 `sample_submission.csv`가 현재 디렉토리에 있어야 합니다.\n",
        "# ----------------------------------------------------------------------\n",
        "# 1. 필수 라이브러리 설치 (Colab용)\n",
        "# ----------------------------------------------------------------------\n",
        "!pip install transformers datasets accelerate pandas numpy torch tqdm\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 2. 라이브러리 임포트 및 유틸리티\n",
        "# ----------------------------------------------------------------------\n",
        "import os\n",
        "import random\n",
        "import math\n",
        "from typing import List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
        "\n",
        "# ================================\n",
        "# 0. 설정값 (하이퍼파라미터 튜닝 영역)\n",
        "# ================================\n",
        "# v2-50m 모델 사용\n",
        "MODEL_NAME = \"InstaDeepAI/nucleotide-transformer-v2-50m-multi-species\"\n",
        "\n",
        "TEST_PATH = \"test.csv\"\n",
        "SAMPLE_SUB_PATH = \"sample_submission.csv\"\n",
        "OUTPUT_PATH = \"submission_ntv2_boosted_contrastive.csv\"\n",
        "\n",
        "MAX_SEQ_LEN = 512                                # 토큰 최대 길이\n",
        "BASE_HIDDEN_DIM = 512                            # v2-50m 모델의 기본 hidden_size\n",
        "EMBED_DIM = BASE_HIDDEN_DIM * 2                  # Mean + Max Pooling으로 1024차원 사용\n",
        "\n",
        "DO_TRAIN = True                                  # 파인튜닝 여부 (대회에서는 True 권장)\n",
        "MAX_TRAIN_SEQS = 20000                           # test 중 학습에 쓸 최대 시퀀스 수 (속도 고려)\n",
        "EPOCHS = 1                                       # Colab 환경 최적화\n",
        "BATCH_SIZE = 8\n",
        "LR = 1e-5                                        # 학습률 조정 (1e-5 ~ 3e-5)\n",
        "WARMUP_RATIO = 0.05\n",
        "\n",
        "# Triplet Contrastive Learning 설정\n",
        "# Mut_A: 작은 변이 (Positive 역할), Mut_B: 큰 변이 (Negative 역할)\n",
        "MUTATION_LEVEL_A = 0.002                         # 0.2% SNV (작은 변이)\n",
        "MUTATION_LEVEL_B = 0.01                          # 1.0% SNV (큰 변이)\n",
        "\n",
        "# Triplet Loss Margin: dist(Anchor, Positive) + MARGIN < dist(Anchor, Negative)\n",
        "TRIPLET_MARGIN = 0.2\n",
        "ALPHA_MARGIN_SCALE = 1.0 # 동적 마진 스케일 조정 (PCC 개선)\n",
        "\n",
        "SEED = 2025\n",
        "\n",
        "# ================================\n",
        "# 1. 유틸 함수들\n",
        "# ================================\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def mutate_sequence_snvs(seq: str, mutation_ratio: float) -> Tuple[str, int]:\n",
        "    \"\"\"\n",
        "    주어진 DNA 염기열에 대해 SNV(single nucleotide variants)만 랜덤으로 넣어서\n",
        "    '조금 다른' variant 시퀀스를 만든다.\n",
        "    \"\"\"\n",
        "    bases = [\"A\", \"C\", \"G\", \"T\"]\n",
        "    seq = seq.upper()\n",
        "    length = len(seq)\n",
        "\n",
        "    # 최소 1개 변이 보장\n",
        "    num_mutations = max(1, int(length * mutation_ratio))\n",
        "\n",
        "    if num_mutations >= length:\n",
        "        num_mutations = length // 2 if length >= 2 else 1\n",
        "\n",
        "    positions = random.sample(range(length), num_mutations)\n",
        "    seq_list = list(seq)\n",
        "\n",
        "    for pos in positions:\n",
        "        original = seq_list[pos]\n",
        "        candidates = [b for b in bases if b != original]\n",
        "        if not candidates:\n",
        "            continue\n",
        "        seq_list[pos] = random.choice(candidates)\n",
        "\n",
        "    mutated = \"\".join(seq_list)\n",
        "    return mutated, num_mutations\n",
        "\n",
        "def get_pooled_embedding(last_hidden_state: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Mean Pooling과 Max Pooling을 결합하여 (Concatenative Pooling) 임베딩을 추출합니다.\n",
        "    [B, L, H] -> [B, 2*H]\n",
        "    \"\"\"\n",
        "    # 1. Mean Pooling (패딩 제외)\n",
        "    mask = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
        "    masked_hidden = last_hidden_state * mask\n",
        "    summed = masked_hidden.sum(dim=1)\n",
        "    counts = mask.sum(dim=1).clamp(min=1e-6)\n",
        "    mean_emb = summed / counts # [B, H]\n",
        "\n",
        "    # 2. Max Pooling (패딩 제외)\n",
        "    # 패딩 위치는 -inf로 설정하여 Max Pooling 시 선택되지 않도록 함\n",
        "    masked_hidden_max = last_hidden_state.masked_fill(~mask.bool(), -1e9)\n",
        "    max_emb, _ = torch.max(masked_hidden_max, dim=1) # [B, H]\n",
        "\n",
        "    # 3. Concatenate (결합)\n",
        "    return torch.cat((mean_emb, max_emb), dim=1) # [B, 2*H]\n",
        "\n",
        "def cosine_distance(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    코사인 거리: 1 - cos_sim\n",
        "    \"\"\"\n",
        "    return 1.0 - F.cosine_similarity(a, b)\n",
        "\n",
        "# ================================\n",
        "# 2. Dataset 정의 (Triplet Loss 구조)\n",
        "# ================================\n",
        "class MutationContrastiveDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Anchor(Original), Positive(Small Mutation), Negative(Large Mutation) Triplet을 생성\n",
        "    \"\"\"\n",
        "    def __init__(self, seq_list: List[str]):\n",
        "        self.seqs = seq_list\n",
        "        # 작은 변이: Positive 역할을 하여 거리를 가깝게 유도\n",
        "        self.mutation_ratio_P = MUTATION_LEVEL_A\n",
        "        # 큰 변이: Negative 역할을 하여 거리를 멀게 유도\n",
        "        self.mutation_ratio_N = MUTATION_LEVEL_B\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.seqs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq = self.seqs[idx]\n",
        "\n",
        "        # P: Original - Small Mutation (Positive Pair)\n",
        "        mut_P, n_mut_P = mutate_sequence_snvs(seq, self.mutation_ratio_P)\n",
        "\n",
        "        # N: Original - Large Mutation (Negative Pair)\n",
        "        mut_N, n_mut_N = mutate_sequence_snvs(seq, self.mutation_ratio_N)\n",
        "\n",
        "        return {\n",
        "            \"anchor\": seq,\n",
        "            \"positive\": mut_P,\n",
        "            \"negative\": mut_N,\n",
        "            \"num_mut_P\": n_mut_P,\n",
        "            \"num_mut_N\": n_mut_N,\n",
        "        }\n",
        "\n",
        "def collate_fn(batch, tokenizer, max_len: int):\n",
        "    anchors = [b[\"anchor\"] for b in batch]\n",
        "    positives = [b[\"positive\"] for b in batch]\n",
        "    negatives = [b[\"negative\"] for b in batch]\n",
        "\n",
        "    num_mut_P = [b[\"num_mut_P\"] for b in batch]\n",
        "    num_mut_N = [b[\"num_mut_N\"] for b in batch]\n",
        "\n",
        "    # 모든 시퀀스를 한 번에 토큰화\n",
        "    all_seqs = anchors + positives + negatives\n",
        "\n",
        "    enc = tokenizer(\n",
        "        all_seqs,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_len,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "\n",
        "    # 배치 크기\n",
        "    B = len(anchors)\n",
        "\n",
        "    # 결과 분리\n",
        "    enc_A = {k: v[:B] for k, v in enc.items()}\n",
        "    enc_P = {k: v[B:2*B] for k, v in enc.items()}\n",
        "    enc_N = {k: v[2*B:] for k, v in enc.items()}\n",
        "\n",
        "    num_mut_tensor = torch.tensor([num_mut_P, num_mut_N], dtype=torch.float32).T # [B, 2]\n",
        "\n",
        "    return enc_A, enc_P, enc_N, num_mut_tensor\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 3. 모델 아키텍처 및 로드 (Dropout 추가)\n",
        "# ================================\n",
        "class VariantSensitiveGLM(nn.Module):\n",
        "    \"\"\"\n",
        "    기존 AutoModel 위에 Dropout 레이어를 추가하여 Fine-Tuning 안정성 및 일반화 개선\n",
        "    \"\"\"\n",
        "    def __init__(self, model_name: str, hidden_dropout_prob: float = 0.1):\n",
        "        super().__init__()\n",
        "        # AutoModelForMaskedLM 대신, 임베딩 추출에 더 적합한 AutoModel 사용\n",
        "        # ⭐⭐⭐ 수정: ignore_mismatched_sizes=True를 추가하여 MLM 헤드 가중치 불일치 무시 ⭐⭐⭐\n",
        "        self.base_model = AutoModel.from_pretrained(\n",
        "            model_name,\n",
        "            trust_remote_code=True,\n",
        "            ignore_mismatched_sizes=True # MLM 헤드 가중치 불일치 무시\n",
        "        )\n",
        "        # 임베딩 추출 시 안정성을 위해 마지막 히든 스테이트에 Dropout 적용\n",
        "        self.dropout = nn.Dropout(hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.base_model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            output_hidden_states=True,\n",
        "            return_dict=True\n",
        "        )\n",
        "\n",
        "        # 마지막 히든 스테이트에 Dropout 적용\n",
        "        last_hidden_state = self.dropout(outputs.last_hidden_state)\n",
        "\n",
        "        # Concatenative Pooling을 통해 최종 임베딩 벡터 반환\n",
        "        pooled_emb = get_pooled_embedding(last_hidden_state, attention_mask)\n",
        "\n",
        "        return pooled_emb # [B, 2*H]\n",
        "\n",
        "def load_glm_model(model_name: str, device: torch.device):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "    # tokenizer는 변경 없음\n",
        "    model = VariantSensitiveGLM(model_name) # 여기서 ignore_mismatched_sizes가 적용됨\n",
        "    model.to(device)\n",
        "\n",
        "    return tokenizer, model\n",
        "\n",
        "# ================================\n",
        "# 4. 파인튜닝 루프 (Triplet Loss 적용)\n",
        "# ================================\n",
        "def train_variant_sensitive_glm(\n",
        "    model: VariantSensitiveGLM,\n",
        "    tokenizer: AutoTokenizer,\n",
        "    train_seqs: List[str],\n",
        "    device: torch.device,\n",
        "):\n",
        "    dataset = MutationContrastiveDataset(train_seqs)\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        collate_fn=lambda batch: collate_fn(batch, tokenizer, MAX_SEQ_LEN),\n",
        "    )\n",
        "\n",
        "    num_training_steps = EPOCHS * len(dataloader)\n",
        "    warmup_steps = int(num_training_steps * WARMUP_RATIO)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=warmup_steps,\n",
        "        num_training_steps=num_training_steps,\n",
        "    )\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # 동적 마진 계산: mutation_ratio_N - mutation_ratio_P = 0.01 - 0.002 = 0.008\n",
        "    dynamic_margin = TRIPLET_MARGIN + ALPHA_MARGIN_SCALE * (MUTATION_LEVEL_B - MUTATION_LEVEL_A)\n",
        "    dynamic_margin = torch.tensor(dynamic_margin, dtype=torch.float32).to(device)\n",
        "    print(f\"Triplet Loss Margin: {dynamic_margin.item():.4f}\")\n",
        "\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        epoch_loss = 0.0\n",
        "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "        for step, (enc_A, enc_P, enc_N, num_mut_tensor) in enumerate(progress_bar):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # 1. Anchor, Positive, Negative 임베딩 추출 (Pooling은 모델 내부에서 처리)\n",
        "            # [B, 2*H]\n",
        "            emb_A = model(enc_A[\"input_ids\"].to(device), enc_A[\"attention_mask\"].to(device))\n",
        "            emb_P = model(enc_P[\"input_ids\"].to(device), enc_P[\"attention_mask\"].to(device))\n",
        "            emb_N = model(enc_N[\"input_ids\"].to(device), enc_N[\"attention_mask\"].to(device))\n",
        "\n",
        "            # 2. 코사인 거리 계산\n",
        "            dist_AP = cosine_distance(emb_A, emb_P) # Anchor-Positive 거리 (작아야 함) [B]\n",
        "            dist_AN = cosine_distance(emb_A, emb_N) # Anchor-Negative 거리 (커야 함) [B]\n",
        "\n",
        "            # 3. Triplet Margin Loss 적용\n",
        "            # Triplet Loss: max(0, dist(A, P) - dist(A, N) + margin)\n",
        "\n",
        "            loss_triplet = F.relu(dist_AP - dist_AN + dynamic_margin)\n",
        "            loss = loss_triplet.mean()\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
        "\n",
        "        print(f\"[Epoch {epoch+1}] mean loss = {epoch_loss / len(dataloader):.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# ================================\n",
        "# 5. test.csv 전체 임베딩 추출 (추론)\n",
        "# ================================\n",
        "class TestSeqDataset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame):\n",
        "        self.ids = df[\"ID\"].tolist()\n",
        "        self.seqs = df[\"seq\"].astype(str).tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.ids[idx], self.seqs[idx]\n",
        "\n",
        "\n",
        "def collate_test(batch, tokenizer, max_len: int):\n",
        "    ids = [b[0] for b in batch]\n",
        "    seqs = [b[1] for b in batch]\n",
        "    enc = tokenizer(\n",
        "        seqs,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_len,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    return ids, enc\n",
        "\n",
        "\n",
        "def extract_embeddings(\n",
        "    model: VariantSensitiveGLM,\n",
        "    tokenizer: AutoTokenizer,\n",
        "    test_df: pd.DataFrame,\n",
        "    device: torch.device,\n",
        ") -> pd.DataFrame:\n",
        "    model.eval()\n",
        "    dataset = TestSeqDataset(test_df)\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=BATCH_SIZE * 4, # 추론 시 배치 사이즈 증대\n",
        "        shuffle=False,\n",
        "        collate_fn=lambda batch: collate_test(batch, tokenizer, MAX_SEQ_LEN),\n",
        "    )\n",
        "\n",
        "    all_ids = []\n",
        "    all_embs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for ids, enc in tqdm(dataloader, desc=\"Extracting embeddings\"):\n",
        "            input_ids = enc[\"input_ids\"].to(device)\n",
        "            attn_mask = enc[\"attention_mask\"].to(device)\n",
        "\n",
        "            # 모델의 forward 함수는 이미 Concatenative Pooling된 [B, 2*H] 임베딩을 반환\n",
        "            emb = model(input_ids, attn_mask)\n",
        "\n",
        "            emb = emb.detach().cpu().numpy()\n",
        "            all_ids.extend(ids)\n",
        "            all_embs.append(emb)\n",
        "\n",
        "    all_embs = np.vstack(all_embs)  # [N, EMBED_DIM]\n",
        "\n",
        "    # submission 포맷으로 변환\n",
        "    sub = pd.read_csv(SAMPLE_SUB_PATH)\n",
        "\n",
        "    # ID 순서에 맞춰서 정렬\n",
        "    id_to_index = {id_: i for i, id_ in enumerate(all_ids)}\n",
        "    # test_df의 크기로 배열 생성\n",
        "    ordered_embs = np.zeros((len(test_df), EMBED_DIM), dtype=np.float32)\n",
        "\n",
        "    # test_df 순서로 정렬\n",
        "    for i, id_ in enumerate(test_df[\"ID\"].tolist()):\n",
        "        idx = id_to_index[id_]\n",
        "        ordered_embs[i] = all_embs[idx]\n",
        "\n",
        "    emb_cols = [f\"emb_{i:04d}\" for i in range(EMBED_DIM)]\n",
        "    emb_df = pd.DataFrame(ordered_embs, columns=emb_cols)\n",
        "    out_df = pd.concat([test_df[[\"ID\"]], emb_df], axis=1) # ID와 임베딩 결합\n",
        "\n",
        "    return out_df\n",
        "\n",
        "# ================================\n",
        "# 6. 메인 실행부\n",
        "# ================================\n",
        "def main():\n",
        "    set_seed(SEED)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "    print(f\"Final Embedding Dimension: {EMBED_DIM}\")\n",
        "\n",
        "    # 1. 데이터 로드\n",
        "    test_df = pd.read_csv(TEST_PATH)\n",
        "    print(\"test shape:\", test_df.shape)\n",
        "\n",
        "    # 2. gLM 로드\n",
        "    tokenizer, model = load_glm_model(MODEL_NAME, device)\n",
        "\n",
        "    # 3. ---------- 파인튜닝 (Self-Supervised Contrastive) ----------\n",
        "    if DO_TRAIN:\n",
        "        # 학습에 사용할 시퀀스 준비\n",
        "        uniq_seqs = test_df[\"seq\"].astype(str).unique().tolist()\n",
        "        random.shuffle(uniq_seqs)\n",
        "        train_seqs = uniq_seqs[:MAX_TRAIN_SEQS] if len(uniq_seqs) > MAX_TRAIN_SEQS else uniq_seqs\n",
        "\n",
        "        print(f\"Train sequences (randomly selected from test.csv): {len(train_seqs)}\")\n",
        "        model = train_variant_sensitive_glm(\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            train_seqs=train_seqs,\n",
        "            device=device,\n",
        "        )\n",
        "\n",
        "    # 4. ---------- 임베딩 추출 ----------\n",
        "    submission_df = extract_embeddings(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        test_df=test_df,\n",
        "        device=device,\n",
        "    )\n",
        "\n",
        "    # 5. 저장\n",
        "    submission_df.to_csv(OUTPUT_PATH, index=False)\n",
        "    print(\"Saved submission to:\", OUTPUT_PATH)\n",
        "    print(\"\\n--- Final Submission Head ---\")\n",
        "    print(submission_df.head())\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "-C-XJ4I52ppN"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}